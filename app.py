import os
import streamlit as st
import faiss
import numpy as np
from openai import OpenAI

# ========== CONFIG ==========
from dotenv import load_dotenv

load_dotenv(".env")
client = OpenAI(
    base_url="https://api.fanar.qa",
    api_key= os.getenv("FANAR_API_KEY", "FANAR_API_KEY not found"),
)
MODEL_NAME = "Fanar"

DOCUMENT_FOLDER = "./data"
INDEX_PATH = "./database/faiss.index"
CHUNK_PATH = "./database/chunks.pkl"
READ_FILE_LIST = "./database/read_files.json"
PROMPT_PATH = "prompts/system_prompt.txt"

# ========== CONTENT LOADING ==========
from langchain.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

def load_document(docu_path):
    document = None
    if docu_path.endswith(".txt"):
        loader = TextLoader(docu_path, encoding="utf-8")
        document = loader.load()
    if docu_path.endswith(".pdf"):
        loader = PyPDFLoader(docu_path)
        document = loader.load()
    return document

def split_document_chunks(document):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " "]
    )
    chunks = splitter.split_documents(document)
    return [chunk.page_content for chunk in chunks]

def load_file_chunks(file_path):
    document = load_document(file_path)
    return split_document_chunks(document)

def load_datafolder_chunks(folder_path):
#     all_chunks = []
#     for filename in os.listdir(folder_path):
#         all_chunks.extend(load_file_chunks(os.path.join(folder_path, filename)))
#     return all_chunks
    return [chunk for file in os.listdir(folder_path) for chunk in load_file_chunks(os.path.join(folder_path, file))]

import pickle

def save_faiss_index(index, path=INDEX_PATH):
    faiss.write_index(index, path)

def load_faiss_index(path=INDEX_PATH):
    return faiss.read_index(path)

def save_chunks(chunks, path=CHUNK_PATH):
    with open(path, "wb") as f:
        pickle.dump(chunks, f)

def load_chunks(path=CHUNK_PATH):
    with open(path, "rb") as f:
        return pickle.load(f)

import json
from datetime import datetime

def load_read_files(path=READ_FILE_LIST):
    ''' returns a list of read files based on path'''
    if os.path.exists(path):
        with open(path, "r") as f:
            return json.load(f)
    return {}

def save_read_files(seen_files, path=READ_FILE_LIST):
    ''' saves a list of read files on the specified file path'''
    with open(path, "w") as f:
        json.dump(seen_files, f, indent=2)
        
def add_read_file(read_files, file_name):
    read_files[file_name] = {"added_at": datetime.now().isoformat()}

def get_new_files(seen_files, document_folder = DOCUMENT_FOLDER):
    ''' returns a list of files in the speficied folder but not in seen_files'''
    all_files = [f for f in os.listdir(document_folder) if f.endswith((".pdf", ".txt"))]
    return [f for f in all_files if f not in seen_files]


# ========== EMBEDDING + FAISS SETUP ==========
import time

def create_embeddings(text_list, batch_size=30, delay=2.0): # Added batch size because of API limits
    all_embeddings = []
    list_size = len(text_list)

    many_chunks = list_size > 100
    if many_chunks: progress_bar = st.progress(0, text=f"Embedding {list_size} chunks in progress...")
    
    try:
        for i in range(0, list_size, batch_size):
            batch = text_list[i:i + batch_size]
            response = client.embeddings.create(model="Fanar", input=batch)
            embeddings = [r.embedding for r in response.data]
            all_embeddings.extend(embeddings)

            if many_chunks:
                time.sleep(delay) # Delay to avoid rate limit
                percent_done = min((i + batch_size), list_size) / list_size
                progress_bar.progress(percent_done, text=f"Embedding: {i} of {list_size} chunks ({int(percent_done * 100)}%) ")

        if many_chunks: progress_bar.empty() # Remove bar
        return np.array(all_embeddings)
    
    except Exception as e:
        st.error(f"Error in batch {i}â€“{i+batch_size}: {text_list[i]}... \n\n ------ \n\n {e}")
        return None
    
def setup_faiss_index(text_chunks):
    embeddings = create_embeddings(text_chunks)
    if embeddings is None:
        st.error("Failed to create embeddings. Cannot initialize FAISS index.")
        return None, None, None  # <â€“â€“ return a tuple regardless of error to avoid more errors
    
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index, chunks, embeddings

def new_document_to_index(new_file_path, index, chunks):
    new_chunks = load_file_chunks(new_file_path)
    new_embeddings = create_embeddings(new_chunks)
    index.add(new_embeddings)
    chunks.extend(new_chunks)

    return index, chunks


# ========== CONTEXT RETRIEVAL ==========

def fetch_relevant_chunks(query, index, chunks, num_chunks=3):
    query_embedding = create_embeddings([query])
    if query_embedding is None:
        return []
    _, indices = index.search(query_embedding, num_chunks)
    return [chunks[i] for i in indices[0]]

# ========== LLM RESPONSE GENERATION ==========

def ask_fanar(chat_history, context_chunks, query):
    context = "\n".join(context_chunks)
    messages = []

    with open(PROMPT_PATH, "r", encoding="utf-8") as file: system_prompt = file.read()

    # System prompt
    messages.append({ "role": "system", "content": system_prompt})

    # Add previous turns
    for turn in chat_history:
        messages.append({"role": "user", "content": turn["user"]})
        messages.append({"role": "assistant", "content": turn["assistant"]})

    # Add current query
    messages.append({"role": "user", "content": query})

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME, 
            messages=messages,
            temperature=0.7,
            max_tokens=600
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"FANAR API error: {e}")
        return "Sorry, something went wrong. Please try again."

# ========== STREAMLIT UI ==========

st.set_page_config(page_title="Mental Health Chatbot Sakina Ai", page_icon="ðŸ§ ")
st.title("ðŸ§  Mental Health Support Chatbot Sakina AI")
st.markdown("_This tool provides general mental health support and is **not** a substitute for professional help. If you're in crisis, please contact a professional or emergency service._")

# Initialize chat_history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Initialize chunks and FAISS index once
if 'chunks' not in st.session_state:

    if not os.path.exists(READ_FILE_LIST):
        with open(READ_FILE_LIST, "w") as f:
            json.dump({}, f)

    seen_files = load_read_files(READ_FILE_LIST)
    new_files = get_new_files(seen_files, DOCUMENT_FOLDER)

    # === Make content DB ======
    if not os.path.exists(INDEX_PATH) and not os.path.exists(CHUNK_PATH):
        st.info("Building new database...")
        chunks = load_datafolder_chunks(DOCUMENT_FOLDER)
        index, chunk_texts, embeddings = setup_faiss_index(chunks)
        save_faiss_index(index, INDEX_PATH)
        save_chunks(chunks, CHUNK_PATH)
        save_read_files(new_files, READ_FILE_LIST)
        new_files = None

    # === Add new documents to content DB ===
    if new_files:
        st.info(f"Found {len(new_files)} new file(s): {', '.join(new_files)}")
        index = load_faiss_index(INDEX_PATH)
        chunks = load_chunks(CHUNK_PATH)

        for file in new_files:
            file_path = os.path.join(DOCUMENT_FOLDER, file)
            index, chunks = new_document_to_index(file_path, index, chunks)
            add_read_file(seen_files, file)
            st.info(f"{file} is added to index")

        save_faiss_index(index, INDEX_PATH)
        save_chunks(chunks, CHUNK_PATH)
        
        save_read_files(seen_files, READ_FILE_LIST)
        # st.success("New documents added to the index.")

    else:
        st.info("Loading cached index and chunks.")

    # === Step 3: Load final state into Streamlit session ===
    st.session_state['faiss_index'] = load_faiss_index(INDEX_PATH)
    st.session_state['chunks'] = load_chunks(CHUNK_PATH)

# User input
user_query = st.text_input("How are you feeling today, or what would you like support with?")

# Display chatbot response
if st.button("Start Chat"):
    if user_query.strip() and 'faiss_index' in st.session_state:
        context_chunks = fetch_relevant_chunks(user_query, st.session_state['faiss_index'], st.session_state['chunks'])
        if context_chunks:
            answer = ask_fanar(st.session_state.chat_history, context_chunks, user_query)
            st.session_state.chat_history.append({
                "user": user_query,
                "assistant": answer
            })

            # Display all messages so far
            for turn in st.session_state.chat_history:
                st.markdown(f"**You:** {turn['user']}")
                st.markdown(f"**SakinaAI Agent:** {turn['assistant']}")
                st.markdown("---")

        else:
            st.warning("Sorry, I couldn't find relevant context. Please try again.")
    else:
        st.warning("Please enter something you'd like help with.")

if st.button("Reset Conversation"):
    st.session_state.chat_history = []
    st.experimental_rerun()
